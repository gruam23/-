# -*- coding: utf-8 -*-
from DrissionPage import ChromiumPage, ChromiumOptions
import pandas as pd
import time
import random
import os
from urllib.parse import urljoin

# ================= é…ç½®åŒºåŸŸ =================
TARGET_URL = "https://ningbo.chinatax.gov.cn/zcwj/zcfgk/index.html"
VERSION = "v12.0 (æé€Ÿæ ‡é¢˜ç‰ˆ - æ‹’ç»æ— æ•ˆç­‰å¾…)"


def get_desktop_path():
    return os.path.join(os.path.expanduser("~"), "Desktop")


OUTPUT_FILE = os.path.join(get_desktop_path(), "å®æ³¢ç¨åŠ¡_æ”¿ç­–æ³•è§„åº“_å…¨é‡æŠ“å–.xlsx")


# ================= æ ¸å¿ƒé€»è¾‘ =================

def extract_detail(tab):
    """
    è¿›å…¥è¯¦æƒ…é¡µåï¼ŒåŒæ—¶æå–ï¼šå®Œæ•´æ ‡é¢˜ã€å…ƒæ•°æ®ã€æ­£æ–‡ã€é™„ä»¶
    """
    try:
        info = {
            "æ ‡é¢˜": "", "æ­£æ–‡": "", "æ–‡å·": "", "å‘æ–‡å•ä½": "", "å‘å¸ƒæ—¥æœŸ": "", "é™„ä»¶": []
        }

        # === ğŸŒŸ æ ¸å¿ƒæé€Ÿä¼˜åŒ–ï¼šé™åˆ¶æŸ¥æ‰¾æ—¶é—´ ===
        try:
            # åªç»™ 0.2 ç§’çš„æ—¶é—´æ‰¾æ ‡é¢˜ï¼Œæ‰¾ä¸åˆ°ç«‹åˆ»æ¢ä¸‹ä¸€ä¸ªç­–ç•¥
            title_ele = tab.ele('tag:h1', timeout=0.2)

            if not title_ele:
                title_ele = tab.ele('.title', timeout=0.2)

            if not title_ele:
                title_ele = tab.ele('#title', timeout=0.2)

            if title_ele:
                info["æ ‡é¢˜"] = title_ele.text.strip()
        except:
            pass

        # === 1. Metaæ•°æ® ===
        try:
            # Meta æ•°æ®é€šå¸¸åœ¨å¤´éƒ¨ï¼Œä¸éœ€è¦ç­‰å¾…
            date_ele = tab.ele('xpath://meta[@name="PubDate"]', timeout=0.2)
            if date_ele: info["å‘å¸ƒæ—¥æœŸ"] = date_ele.attr("content").split(" ")[0]
            source_ele = tab.ele('xpath://meta[@name="ContentSource"]', timeout=0.2)
            if source_ele: info["å‘æ–‡å•ä½"] = source_ele.attr("content")
        except:
            pass

        # === 2. æ­£æ–‡ (è¿™æ˜¯å¿…é¡»å­˜åœ¨çš„ï¼Œå¯ä»¥å¤šç­‰ä¸€ä¼šç¡®ä¿åŠ è½½) ===
        content_ele = tab.ele('#zoom', timeout=5)
        if content_ele:
            info["æ­£æ–‡"] = content_ele.text
        else:
            info["æ­£æ–‡"] = tab.ele('.info-cont').text if tab.ele('.info-cont') else "æ­£æ–‡æå–å¤±è´¥"

        # === 3. æ–‡å·è¡¥æ•‘ ===
        if not info["æ–‡å·"]:
            first_part = info["æ­£æ–‡"][:300]
            if "å‘å¸ƒæ–‡å·" in first_part:
                try:
                    parts = first_part.split("å‘å¸ƒæ–‡å·")
                    candidate = parts[1].split("\n")[0].replace("ã€‘", "").replace(":", "").replace("ï¼š", "").strip()
                    info["æ–‡å·"] = candidate
                except:
                    pass

        # === 4. é™„ä»¶ (å¿«é€Ÿæ‰«æ) ===
        # ä¸éœ€è¦ waitï¼Œç›´æ¥è·å–å½“å‰å·²åŠ è½½çš„
        links = tab.eles('tag:a')
        for link in links:
            href = link.attr('href')
            if not href: continue
            if href.endswith(('.doc', '.docx', '.xls', '.xlsx', '.pdf', '.zip', '.rar')):
                full_url = urljoin(tab.url, href)
                info["é™„ä»¶"].append({
                    "æ–‡ä»¶å": link.text,
                    "é“¾æ¥": full_url
                })
        return info

    except Exception as e:
        print(f"    âŒ è¯¦æƒ…é¡µè§£æå‡ºé”™: {e}")
        return {}


def save_to_excel(data_list, filepath):
    if not data_list: return
    while True:
        try:
            df_new = pd.DataFrame(data_list)
            if os.path.exists(filepath):
                try:
                    with pd.ExcelWriter(filepath, mode='a', engine='openpyxl', if_sheet_exists='overlay') as writer:
                        pass
                    df_old = pd.read_excel(filepath, engine="openpyxl")
                    df = pd.concat([df_old, df_new], ignore_index=True)
                    df.drop_duplicates(subset=["é“¾æ¥", "é™„ä»¶é“¾æ¥"], keep="last", inplace=True)
                except PermissionError:
                    raise PermissionError
                except:
                    df = df_new
            else:
                df = df_new

            cols = ["æ ‡é¢˜", "å‘å¸ƒæ—¥æœŸ", "å‘æ–‡å•ä½", "æ–‡å·", "æ­£æ–‡", "é™„ä»¶æ–‡ä»¶å", "é™„ä»¶é“¾æ¥", "é“¾æ¥"]
            for c in cols:
                if c not in df.columns: df[c] = ""
            df = df[cols]
            df.to_excel(filepath, index=False, engine="openpyxl")
            print(f"   ğŸ’¾ å·²ä¿å­˜ (æ€»è¡Œæ•°: {len(df)})")
            break
        except PermissionError:
            print("\nğŸš¨ é”™è¯¯ï¼šExcel æ–‡ä»¶è¢«å ç”¨ï¼è¯·å…³é—­æ–‡ä»¶...")
            time.sleep(5)
        except Exception as e:
            print(f"   âŒ Excelä¿å­˜æœªçŸ¥å¤±è´¥: {e}")
            break


def main():
    print(f"ğŸš€ å¯åŠ¨é‡‡é›†å™¨ - {VERSION}")

    co = ChromiumOptions()
    co.set_user_agent(
        user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
    # ä¿æŒç¦å›¾ï¼Œè¿½æ±‚æè‡´é€Ÿåº¦
    co.set_argument('--blink-settings=imagesEnabled=false')
    co.set_argument('--mute-audio')
    co.set_argument('--window-position=-3000,-3000')  # ç§»å‡ºå±å¹•
    co.ignore_certificate_errors()

    page = ChromiumPage(addr_or_opts=co)

    print(f"ğŸŒ æ­£åœ¨è®¿é—®: {TARGET_URL}")
    page.get(TARGET_URL)
    time.sleep(2)

    processed_urls = set()
    if os.path.exists(OUTPUT_FILE):
        try:
            try:
                df = pd.read_excel(OUTPUT_FILE, engine="openpyxl")
                processed_urls = set(df["é“¾æ¥"].dropna().tolist())
                print(f"ğŸ“š å·²è¯»å– {len(processed_urls)} æ¡å†å²è®°å½•")
            except:
                pass
        except:
            pass

    page_num = 1
    empty_page_count = 0

    while True:
        print(f"\nğŸ”„ æ­£åœ¨å¤„ç†ç¬¬ {page_num} é¡µ...")

        try:
            page.wait.ele('tag:a', timeout=8)
        except:
            pass

        all_links = page.eles('tag:a')
        article_links = []
        for link in all_links:
            url = link.attr('href')
            list_title = link.text

            if not url or "javascript" in url: continue
            if not list_title or len(list_title) < 5: continue

            is_article = ("/art/" in url) or ("/content/" in url) or ("202" in url)
            is_category = url.endswith("index.html")

            if is_article and not is_category:
                if url not in processed_urls:
                    article_links.append({"title": list_title, "url": url})

        unique_links = []
        seen = set()
        for item in article_links:
            if item['url'] not in seen:
                unique_links.append(item)
                seen.add(item['url'])

        if not unique_links:
            print("âš ï¸ æœ¬é¡µæœªå‘ç°æ–°æ•°æ®ã€‚")
            empty_page_count += 1
            if empty_page_count >= 3:
                print("ğŸ›‘ è¿ç»­ 3 é¡µæ— æ•°æ®ï¼Œåˆ¤æ–­ä¸ºç»“æŸã€‚")
                break
        else:
            print(f"   ğŸ“„ ç­›é€‰å‡º {len(unique_links)} ç¯‡æ–°æ–‡ç« ")
            empty_page_count = 0

        # === æŠ“å–å¾ªç¯ ===
        for item in unique_links:
            short_title = item['title']
            print(f"   Downloading: {short_title[:15]}...")

            try:
                new_tab = page.new_tab(item["url"])
                # ç­‰å¾…æ­£æ–‡åŠ è½½ (è¿™æ˜¯å”¯ä¸€éœ€è¦èŠ±æ—¶é—´ç­‰çš„)
                new_tab.ele('#zoom', timeout=8)

                detail = extract_detail(new_tab)
                new_tab.close()

                final_title = detail.get("æ ‡é¢˜")
                if not final_title:
                    final_title = short_title

                row_base = {
                    "æ ‡é¢˜": final_title,
                    "é“¾æ¥": item["url"],
                    "å‘å¸ƒæ—¥æœŸ": detail.get("å‘å¸ƒæ—¥æœŸ", ""),
                    "å‘æ–‡å•ä½": detail.get("å‘æ–‡å•ä½", ""),
                    "æ–‡å·": detail.get("æ–‡å·", ""),
                    "æ­£æ–‡": detail.get("æ­£æ–‡", "")
                }

                current_data = []
                if detail["é™„ä»¶"]:
                    for att in detail["é™„ä»¶"]:
                        row = row_base.copy()
                        row["é™„ä»¶æ–‡ä»¶å"] = att["æ–‡ä»¶å"]
                        row["é™„ä»¶é“¾æ¥"] = att["é“¾æ¥"]
                        current_data.append(row)
                else:
                    row_base["é™„ä»¶æ–‡ä»¶å"] = ""
                    row_base["é™„ä»¶é“¾æ¥"] = ""
                    current_data.append(row_base)

                processed_urls.add(item["url"])
                save_to_excel(current_data, OUTPUT_FILE)
                # å‡ ä¹æ— å»¶è¿Ÿçš„è¿ç»­æŠ“å–
                time.sleep(0.01)
            except Exception as e:
                print(f"   âŒ: {e}")
                if page.tabs_count > 1: page.close_tabs(page.tab_ids[1:])

                # === ç¿»é¡µ (åŠ é€Ÿç‰ˆ) ===
        print("ğŸ‘† ç¿»é¡µä¸­...")
        try:
            right_box = page.ele('.right-box')
            if right_box:
                next_btn = right_box.ele('.layui-laypage-next')
            else:
                next_btn = page.ele('.layui-laypage-next')

            if next_btn:
                class_val = next_btn.attr("class")
                if class_val and "disabled" in class_val:
                    print(f"ğŸ›‘ æŒ‰é’®å˜ç°ï¼Œç»“æŸ (å…± {page_num} é¡µ)")
                    break

                next_btn.click(by_js=True)

                # ğŸŒŸ ä» 3ç§’ ç¼©çŸ­åˆ° 1.5ç§’ï¼Œè¶³å¤Ÿç½‘é¡µåˆ·æ–°äº†
                print("   â³ ç­‰å¾…åˆ·æ–° (1.5s)...")
                time.sleep(1.5)

                page_num += 1
            else:
                print("ğŸ›‘ æœªæ‰¾åˆ°ç¿»é¡µæŒ‰é’®ï¼Œç»“æŸã€‚")
                break

        except Exception as e:
            print(f"ğŸ›‘ ç¿»é¡µæµç¨‹å‡ºé”™: {e}")
            break

    print(f"\nğŸ‰ å®Œæˆï¼æ–‡ä»¶: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()